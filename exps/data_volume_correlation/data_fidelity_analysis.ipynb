{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Fidelity and SQL Fidelity Correlation Analysis\n",
        "\n",
        "This notebook integrates data volume and SQL subset analysis:\n",
        "1. Merge two data files\n",
        "2. Calculate data volume fidelity correlations\n",
        "3. Perform SQL subset selection analysis\n",
        "4. Visualize comparison between two approaches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Data Merging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
        "import warnings\n",
        "\n",
        "print(\"Starting data merging and analysis...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file1 = \"./20251022_033707/config_validation_results.csv\"\n",
        "file2 = \"./20251022_033810/config_validation_results.csv\"\n",
        "output_path = \"./merged_results.csv\"\n",
        "\n",
        "def merge_csv_files(file1, file2, output_path):\n",
        "    print(f\"Reading file: {file1}\")\n",
        "    df1 = pd.read_csv(file1)\n",
        "    print(f\"File 1: {len(df1)} records\")\n",
        "    \n",
        "    print(f\"Reading file: {file2}\")\n",
        "    df2 = pd.read_csv(file2)\n",
        "    print(f\"File 2: {len(df2)} records\")\n",
        "    \n",
        "    max_config_id_df1 = df1['config_id'].max()\n",
        "    print(f\"File 1 max config_id: {max_config_id_df1}\")\n",
        "    \n",
        "    df2['config_id'] = df2['config_id'] + max_config_id_df1 + 1\n",
        "    print(f\"File 2 config_id range after adjustment: {df2['config_id'].min()} - {df2['config_id'].max()}\")\n",
        "    \n",
        "    merged_df = pd.concat([df1, df2], ignore_index=True)\n",
        "    print(f\"Total records after merging: {len(merged_df)}\")\n",
        "    \n",
        "    merged_df.to_csv(output_path, index=False)\n",
        "    print(f\"Merged results saved to: {output_path}\")\n",
        "    \n",
        "    return merged_df\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    merged_df = pd.read_csv(output_path)\n",
        "else:\n",
        "    merged_df = merge_csv_files(file1, file2, output_path)\n",
        "    print(f\"\\nMerging complete! Total records: {len(merged_df)}\")\n",
        "    print(f\"Fidelity levels: {sorted(merged_df['fidelity'].unique())}\")\n",
        "    print(f\"Databases: {merged_df['database'].unique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Volume Fidelity Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_fidelity_correlations(df, method='spearman'):\n",
        "    full_fidelity = 1.0\n",
        "    full_data = df[df['fidelity'] == full_fidelity].copy()\n",
        "    print(f\"Full fidelity data: {len(full_data)} configurations\")\n",
        "    \n",
        "    fidelity_levels = sorted([f for f in df['fidelity'].unique() if f != full_fidelity])\n",
        "    print(f\"All fidelity levels: {fidelity_levels}\")\n",
        "    \n",
        "    correlations = {}\n",
        "    \n",
        "    for fidelity in fidelity_levels:\n",
        "        fidelity_data = df[df['fidelity'] == fidelity].copy()\n",
        "        \n",
        "        common_configs = set(full_data['config_id']) & set(fidelity_data['config_id'])\n",
        "        print(f\"fidelity={fidelity}: Found {len(common_configs)} common configurations\")\n",
        "        \n",
        "        if len(common_configs) < 3:\n",
        "            print(f\"fidelity={fidelity}: Insufficient common configurations ({len(common_configs)})\")\n",
        "            continue\n",
        "        \n",
        "        full_subset = full_data[full_data['config_id'].isin(common_configs)].copy()\n",
        "        fidelity_subset = fidelity_data[fidelity_data['config_id'].isin(common_configs)].copy()\n",
        "        \n",
        "        full_subset = full_subset.reset_index(drop=True)\n",
        "        fidelity_subset = fidelity_subset.reset_index(drop=True)\n",
        "        \n",
        "        full_subset_copy = full_subset.copy()\n",
        "        fidelity_subset_copy = fidelity_subset.copy()\n",
        "        \n",
        "        incomplete_records = []\n",
        "        if 'successful_files' in full_subset_copy.columns and 'total_files' in full_subset_copy.columns:\n",
        "            for idx, row in full_subset_copy.iterrows():\n",
        "                if row['successful_files'] < row['total_files']:\n",
        "                    completion_ratio = row['successful_files'] / row['total_files']\n",
        "                    penalty_factor = np.exp(2.0 * (1.0 - completion_ratio))\n",
        "                    original_obj = row['objective']\n",
        "                    full_subset_copy.loc[idx, 'objective'] = original_obj * penalty_factor\n",
        "                    incomplete_records.append((row['config_id'], completion_ratio, original_obj, row['objective']))\n",
        "        \n",
        "        if 'successful_files' in fidelity_subset_copy.columns and 'total_files' in fidelity_subset_copy.columns:\n",
        "            for idx, row in fidelity_subset_copy.iterrows():\n",
        "                if row['successful_files'] < row['total_files']:\n",
        "                    completion_ratio = row['successful_files'] / row['total_files']\n",
        "                    penalty_factor = np.exp(2.0 * (1.0 - completion_ratio))\n",
        "                    original_obj = row['objective']\n",
        "                    fidelity_subset_copy.loc[idx, 'objective'] = original_obj * penalty_factor\n",
        "                    incomplete_records.append((row['config_id'], completion_ratio, original_obj, row['objective']))\n",
        "        \n",
        "        if incomplete_records:\n",
        "            print(f\"  {len(incomplete_records)} records incomplete, applying penalty factor\")\n",
        "            for config_id, ratio, orig, new in incomplete_records[:3]:\n",
        "                print(f\"    Config{config_id}: completion={ratio:.3f}, original={orig:.1f}, penalized={new:.1f}\")\n",
        "        \n",
        "        full_valid = full_subset_copy['objective'].values\n",
        "        fidelity_valid = fidelity_subset_copy['objective'].values\n",
        "        \n",
        "        valid_mask = ~(np.isnan(full_valid) | np.isnan(fidelity_valid) | \n",
        "                    np.isinf(full_valid) | np.isinf(fidelity_valid))\n",
        "        \n",
        "        if valid_mask.sum() < 3:\n",
        "            print(f\"fidelity={fidelity}: Insufficient valid data points ({valid_mask.sum()})\")\n",
        "            continue\n",
        "        \n",
        "        full_valid = full_valid[valid_mask]\n",
        "        fidelity_valid = fidelity_valid[valid_mask]\n",
        "        \n",
        "        if method == 'spearman':\n",
        "            corr, pvalue = spearmanr(full_valid, fidelity_valid)\n",
        "        elif method == 'pearson':\n",
        "            corr, pvalue = pearsonr(full_valid, fidelity_valid)\n",
        "        else:  # kendall\n",
        "            corr, pvalue = kendalltau(full_valid, fidelity_valid)\n",
        "        \n",
        "        full_avg_spark_time = full_subset_copy['spark_time'].mean()\n",
        "        fidelity_avg_spark_time = fidelity_subset_copy['spark_time'].mean()\n",
        "        time_ratio = fidelity_avg_spark_time / full_avg_spark_time if full_avg_spark_time > 0 else 0.0\n",
        "        \n",
        "        full_avg_exec_time = full_subset_copy['elapsed_time'].replace(float('inf'), pd.NA).mean()\n",
        "        fidelity_avg_exec_time = fidelity_subset_copy['elapsed_time'].replace(float('inf'), pd.NA).mean()\n",
        "        exec_time_ratio = fidelity_avg_exec_time / full_avg_exec_time if full_avg_exec_time > 0 and not pd.isna(full_avg_exec_time) else 0.0\n",
        "        \n",
        "        correlations[fidelity] = {\n",
        "            'correlation': corr,\n",
        "            'pvalue': pvalue,\n",
        "            'n_samples': len(full_valid),\n",
        "            'database': fidelity_data['database'].iloc[0] if 'database' in fidelity_data.columns else 'unknown',\n",
        "            'time_stats': {\n",
        "                'full_avg_spark_time': full_avg_spark_time,\n",
        "                'fidelity_avg_spark_time': fidelity_avg_spark_time,\n",
        "                'time_ratio': time_ratio,\n",
        "                'full_avg_exec_time': full_avg_exec_time,\n",
        "                'fidelity_avg_exec_time': fidelity_avg_exec_time,\n",
        "                'exec_time_ratio': exec_time_ratio\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nfidelity={fidelity} ({correlations[fidelity]['database']}):\")\n",
        "        print(f\"  Correlation: {corr:.4f}\")\n",
        "        print(f\"  P-value: {pvalue:.4e}\")\n",
        "        print(f\"  Samples: {len(full_valid)}\")\n",
        "        print(f\"  Time stats:\")\n",
        "        print(f\"    Full fidelity avg spark_time: {full_avg_spark_time:.2f}s\")\n",
        "        print(f\"    Current fidelity avg spark_time: {fidelity_avg_spark_time:.2f}s\")\n",
        "        print(f\"    spark_time ratio: {time_ratio:.4f}\")\n",
        "        print(f\"    Full fidelity avg elapsed_time: {full_avg_exec_time:.2f}s\")\n",
        "        print(f\"    Current fidelity avg elapsed_time: {fidelity_avg_exec_time:.2f}s\")\n",
        "        print(f\"    elapsed_time ratio: {exec_time_ratio:.4f}\")\n",
        "    \n",
        "    return correlations\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Data Volume Fidelity Correlation Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "method = 'kendall'\n",
        "data_volume_correlations = calculate_fidelity_correlations(merged_df, method=method)\n",
        "correlation_file = f\"./dv_{method}_corr.json\"\n",
        "with open(correlation_file, 'w') as f:\n",
        "    json.dump(data_volume_correlations, f, indent=2, ensure_ascii=False)\n",
        "print(f\"\\nCorrelation results saved to: {correlation_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: SQL Subset Selection Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "sys.path.append('.')\n",
        "\n",
        "from sql_subset_selection import (\n",
        "    compute_subset_correlation,\n",
        "    multi_fidelity_sql_selection,\n",
        "    multi_fidelity_sql_selection_incremental,\n",
        "    analyze_fidelity_subsets\n",
        ")\n",
        "\n",
        "print(\"SQL analysis functions imported\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_sql_analysis(df, method='spearman', time_type='spark_time', sql_type='qt', tolerance=0.1):\n",
        "    print(\"=\" * 60)\n",
        "    print(\"SQL Subset Selection Analysis\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    df_full = df[df['fidelity'] == 1.0].copy()\n",
        "    print(f\"Using full fidelity data: {len(df_full)} configurations\")\n",
        "    print(f\"Time type: {time_type}, SQL type: {sql_type}, Tolerance: {tolerance}\")\n",
        "    \n",
        "    fidelity_levels = [1/27, 1/9, 1/3, 3/5, 1.0]\n",
        "    \n",
        "    print(\"\\n--- Non-incremental SQL Selection ---\")\n",
        "    fidelity_subsets_non_inc, sql_stats = multi_fidelity_sql_selection(\n",
        "        df_full, \n",
        "        fidelity_levels=fidelity_levels,\n",
        "        lambda_penalty=0.1,\n",
        "        correlation_method=method,\n",
        "        time_type=time_type,\n",
        "        sql_type=sql_type,\n",
        "        tolerance=tolerance\n",
        "    )\n",
        "    \n",
        "    print(\"\\n--- Incremental SQL Selection ---\")\n",
        "    fidelity_subsets_inc, _ = multi_fidelity_sql_selection_incremental(\n",
        "        df_full,\n",
        "        fidelity_levels=fidelity_levels,\n",
        "        lambda_penalty=0.1,\n",
        "        correlation_method=method,\n",
        "        time_type=time_type,\n",
        "        sql_type=sql_type,\n",
        "        tolerance=tolerance\n",
        "    )\n",
        "    \n",
        "    print(\"\\n--- Non-incremental Analysis ---\")\n",
        "    analyze_fidelity_subsets(fidelity_subsets_non_inc, sql_stats, df_full, method)\n",
        "    \n",
        "    print(\"\\n--- Incremental Analysis ---\")\n",
        "    analyze_fidelity_subsets(fidelity_subsets_inc, sql_stats, df_full, method)\n",
        "\n",
        "    save_sql_selection_results(fidelity_subsets_non_inc, fidelity_subsets_inc, sql_stats, method, time_type, sql_type)\n",
        "    \n",
        "    return fidelity_subsets_non_inc, fidelity_subsets_inc, sql_stats\n",
        "\n",
        "def save_sql_selection_results(fidelity_subsets_non_inc, fidelity_subsets_inc, sql_stats, method, time_type, sql_type):\n",
        "    \"\"\"Save SQL selection results with detailed statistics to JSON file\"\"\"\n",
        "    \n",
        "    def calculate_subset_statistics(fidelity_subsets, df_full, sql_stats, method):\n",
        "        \"\"\"Calculate detailed statistics for each fidelity subset\"\"\"\n",
        "        subset_stats = {}\n",
        "        \n",
        "        for fidelity, sqls in fidelity_subsets.items():\n",
        "            if not sqls:\n",
        "                continue\n",
        "                \n",
        "            total_estimated_time = sum(sql_stats[sql]['estimated_time'] for sql in sqls)\n",
        "            subset_corr = compute_subset_correlation(df_full, sqls, sql_stats, method)\n",
        "\n",
        "            sql_details = {}\n",
        "            for sql in sqls:\n",
        "                if sql in sql_stats:\n",
        "                    sql_details[sql] = {\n",
        "                        'correlation': sql_stats[sql]['correlation'],\n",
        "                        'estimated_time': sql_stats[sql]['estimated_time'],\n",
        "                        'avg_time': sql_stats[sql]['avg_time'],\n",
        "                        'total_time': sql_stats[sql]['total_time']\n",
        "                    }\n",
        "            \n",
        "            # Calculate correlation statistics\n",
        "            correlations = [sql_stats[sql]['correlation'] for sql in sqls if sql in sql_stats]\n",
        "            time_ratios = [sql_stats[sql]['estimated_time'] for sql in sqls if sql in sql_stats]\n",
        "            \n",
        "            subset_stats[fidelity] = {\n",
        "                'sql_count': len(sqls),\n",
        "                'sql_list': list(sqls),\n",
        "                'time_ratio': total_estimated_time,\n",
        "                'subset_correlation': subset_corr,\n",
        "                'sql_details': sql_details,\n",
        "                'correlation_stats': {\n",
        "                    'mean': np.mean(correlations) if correlations else 0,\n",
        "                    'std': np.std(correlations) if correlations else 0,\n",
        "                    'min': np.min(correlations) if correlations else 0,\n",
        "                    'max': np.max(correlations) if correlations else 0\n",
        "                },\n",
        "                'time_stats': {\n",
        "                    'mean': np.mean(time_ratios) if time_ratios else 0,\n",
        "                    'std': np.std(time_ratios) if time_ratios else 0,\n",
        "                    'min': np.min(time_ratios) if time_ratios else 0,\n",
        "                    'max': np.max(time_ratios) if time_ratios else 0\n",
        "                }\n",
        "            }\n",
        "        \n",
        "        return subset_stats\n",
        "    \n",
        "    non_inc_stats = calculate_subset_statistics(fidelity_subsets_non_inc, merged_df[merged_df['fidelity'] == 1.0], sql_stats, method)\n",
        "    inc_stats = calculate_subset_statistics(fidelity_subsets_inc, merged_df[merged_df['fidelity'] == 1.0], sql_stats, method)\n",
        "    \n",
        "    results = {\n",
        "        'metadata': {\n",
        "            'method': method,\n",
        "            'time_type': time_type,\n",
        "            'sql_type': sql_type,\n",
        "            'generation_time': pd.Timestamp.now().isoformat(),\n",
        "            'total_configs': len(merged_df[merged_df['fidelity'] == 1.0]),\n",
        "            'total_sqls': len(sql_stats)\n",
        "        },\n",
        "        'non_incremental': {\n",
        "            'fidelity_subsets': {k: list(v) for k, v in fidelity_subsets_non_inc.items()},\n",
        "            'statistics': non_inc_stats\n",
        "        },\n",
        "        'incremental': {\n",
        "            'fidelity_subsets': {k: list(v) for k, v in fidelity_subsets_inc.items()},\n",
        "            'statistics': inc_stats\n",
        "        },\n",
        "        'sql_stats': sql_stats\n",
        "    }\n",
        "    \n",
        "    output_path = f\"./sql_{method}_{sql_type}.json\"\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    print(f\"\\nSQL selection results saved to: {output_path}\")\n",
        "    print(f\"  - Non-incremental: {len(fidelity_subsets_non_inc)} fidelity levels\")\n",
        "    print(f\"  - Incremental: {len(fidelity_subsets_inc)} fidelity levels\")\n",
        "    print(f\"  - Total SQLs analyzed: {len(sql_stats)}\")\n",
        "\n",
        "method = 'kendall'\n",
        "time_type = 'spark_time'\n",
        "sql_type = 'qt'\n",
        "tolerance = 0.05\n",
        "fidelity_subsets_non_inc, fidelity_subsets_inc, sql_stats = perform_sql_analysis(merged_df, method, time_type, sql_type, tolerance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Visualization and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_visualization_data(data_volume_correlations, sql_stats_path):\n",
        "    data_volume_data = []\n",
        "    for fidelity, stats in data_volume_correlations.items():\n",
        "        data_volume_data.append({\n",
        "            'fidelity': fidelity,\n",
        "            'time_ratio': stats['time_stats']['time_ratio'],\n",
        "            'correlation': stats['correlation'],\n",
        "            'type': 'Data Volume'\n",
        "        })\n",
        "    \n",
        "    data_volume_data.append({\n",
        "        'fidelity': 1.0,\n",
        "        'time_ratio': 1.0,\n",
        "        'correlation': 1.0,\n",
        "        'type': 'Data Volume'\n",
        "    })\n",
        "\n",
        "    with open(sql_stats_path, 'r') as f:\n",
        "        sql_stats = json.load(f)\n",
        "    sql_non_inc_data = []\n",
        "    for key, data_dict in sql_stats['non_incremental']['statistics'].items():\n",
        "        sql_non_inc_data.append({\n",
        "            'fidelity': key,\n",
        "            'time_ratio': data_dict['time_ratio'],\n",
        "            'correlation': data_dict['subset_correlation'],\n",
        "            'type': 'SQL Selection (Non-incremental)'\n",
        "        })\n",
        "    \n",
        "    sql_inc_data = []\n",
        "    for key, data_dict in sql_stats['incremental']['statistics'].items():\n",
        "        sql_inc_data.append({\n",
        "            'fidelity': key,\n",
        "            'time_ratio': data_dict['time_ratio'],\n",
        "            'correlation': data_dict['subset_correlation'],\n",
        "            'type': 'SQL Selection (Incremental)'\n",
        "        })\n",
        "    \n",
        "    return data_volume_data, sql_non_inc_data, sql_inc_data\n",
        "\n",
        "method = 'kendall'\n",
        "time_type = 'spark_time'\n",
        "sql_type = 'qt'\n",
        "sql_stats_path = f\"./sql_{method}_{sql_type}.json\"\n",
        "data_volume_data, sql_non_inc_data, sql_inc_data = prepare_visualization_data(\n",
        "    data_volume_correlations,\n",
        "    sql_stats_path\n",
        ")\n",
        "\n",
        "print(\"Data preparation complete\")\n",
        "print(f\"Data volume points: {len(data_volume_data)}\")\n",
        "print(f\"SQL non-incremental points: {len(sql_non_inc_data)}\")\n",
        "print(f\"SQL incremental points: {len(sql_inc_data)}\")\n",
        "\n",
        "# Debug: Print data to verify correlation values\n",
        "print(\"\\nDebug - Data volume data:\")\n",
        "for item in sorted(data_volume_data, key=lambda x: x['fidelity']):\n",
        "    print(f\"  Fidelity: {item['fidelity']}, Time ratio: {item['time_ratio']}, Correlation: {item['correlation']}\")\n",
        "\n",
        "print(\"\\nDebug - SQL non-incremental data:\")\n",
        "for item in sorted(sql_non_inc_data, key=lambda x: x['fidelity']):\n",
        "    print(f\"  Fidelity: {item['fidelity']}, Time ratio: {item['time_ratio']}, Correlation: {item['correlation']}\")\n",
        "\n",
        "print(\"\\nDebug - SQL incremental data:\")\n",
        "for item in sorted(sql_inc_data, key=lambda x: x['fidelity']):\n",
        "    print(f\"  Fidelity: {item['fidelity']}, Time ratio: {item['time_ratio']}, Correlation: {item['correlation']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_plot(data_volume_data, sql_non_inc_data, sql_inc_data):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    \n",
        "    df_volume = pd.DataFrame(data_volume_data)\n",
        "    df_sql_non_inc = pd.DataFrame(sql_non_inc_data)\n",
        "    df_sql_inc = pd.DataFrame(sql_inc_data)\n",
        "    \n",
        "    plt.plot(df_volume['time_ratio'], df_volume['correlation'], \n",
        "             'o-', linewidth=2, markersize=8, label='Data Volume', color='blue')\n",
        "    \n",
        "    plt.plot(df_sql_non_inc['time_ratio'], df_sql_non_inc['correlation'], \n",
        "             's-', linewidth=2, markersize=8, label='SQL Selection (Non-incremental)', color='red')\n",
        "    \n",
        "    plt.plot(df_sql_inc['time_ratio'], df_sql_inc['correlation'], \n",
        "             '^-', linewidth=2, markersize=8, label='SQL Selection (Incremental)', color='green')\n",
        "    \n",
        "    plt.xlabel('Average Time Ratio', fontsize=14)\n",
        "    plt.ylabel('Correlation with Total Execution Time', fontsize=14)\n",
        "    plt.title('Data Volume vs SQL Selection Correlation Comparison', fontsize=16, fontweight='bold')\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Set adaptive y-axis range based on data\n",
        "    all_correlations = list(df_volume['correlation']) + list(df_sql_non_inc['correlation']) + list(df_sql_inc['correlation'])\n",
        "    min_corr = min(all_correlations)\n",
        "    max_corr = max(all_correlations)\n",
        "    y_margin = (max_corr - min_corr) * 0.1  # 10% margin\n",
        "    plt.ylim(max(0, min_corr - y_margin), min(1.0, max_corr + y_margin))\n",
        "    \n",
        "    plt.xlim(0, 1.1)\n",
        "    \n",
        "    # Reference lines for interpretation\n",
        "    plt.axhline(y=0.8, color='gray', linestyle='--', alpha=0.5, label='High Correlation Threshold (0.8)')\n",
        "    plt.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Medium Time Ratio (0.5)')\n",
        "    \n",
        "    for i, row in df_volume.iterrows():\n",
        "        plt.annotate(f'{row[\"correlation\"]:.2f}', \n",
        "                    (row['time_ratio'], row['correlation']),\n",
        "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "    \n",
        "    for i, row in df_sql_non_inc.iterrows():\n",
        "        plt.annotate(f'{row[\"correlation\"]:.2f}', \n",
        "                    (row['time_ratio'], row['correlation']),\n",
        "                    textcoords=\"offset points\", xytext=(0,-15), ha='center')\n",
        "    \n",
        "    for i, row in df_sql_inc.iterrows():\n",
        "        plt.annotate(f'{row[\"correlation\"]:.2f}', \n",
        "                    (row['time_ratio'], row['correlation']),\n",
        "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    output_path = \"fidelity_comparison.png\"\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Comparison plot saved to: {output_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    return df_volume, df_sql_non_inc, df_sql_inc\n",
        "\n",
        "df_volume, df_sql_non_inc, df_sql_inc = create_comparison_plot(\n",
        "    data_volume_data, sql_non_inc_data, sql_inc_data\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_analysis_summary(df_volume, df_sql_non_inc, df_sql_inc):\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Analysis Results Summary\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(\"\\n1. Data Volume Results:\")\n",
        "    print(\"   Fidelity | Time Ratio | Correlation\")\n",
        "    print(\"   ---------|------------|------------\")\n",
        "    for _, row in df_volume.iterrows():\n",
        "        print(f\"   {row['fidelity']:8.2f} | {row['time_ratio']:10.4f} | {row['correlation']:10.4f}\")\n",
        "    \n",
        "    print(\"\\n2. SQL Selection Results (Non-incremental):\")\n",
        "    print(\"   Fidelity | Time Ratio | Correlation\")\n",
        "    print(\"   ---------|------------|------------\")\n",
        "    for _, row in df_sql_non_inc.iterrows():\n",
        "        print(f\"   {float(row['fidelity']):8.2f} | {float(row['time_ratio']):10.4f} | {float(row['correlation']):10.4f}\")\n",
        "    \n",
        "    print(\"\\n3. SQL Selection Results (Incremental):\")\n",
        "    print(\"   Fidelity | Time Ratio | Correlation\")\n",
        "    print(\"   ---------|------------|------------\")\n",
        "    for _, row in df_sql_inc.iterrows():\n",
        "        print(f\"   {float(row['fidelity']):8.2f} | {float(row['time_ratio']):10.4f} | {float(row['correlation']):10.4f}\")\n",
        "    \n",
        "    print(\"\\n4. Efficiency Analysis:\")\n",
        "    \n",
        "    volume_efficiency = df_volume['correlation'] / df_volume['time_ratio']\n",
        "    print(f\"   Data Volume Average Efficiency: {volume_efficiency.mean():.4f}\")\n",
        "    \n",
        "    sql_non_inc_efficiency = df_sql_non_inc['correlation'] / df_sql_non_inc['time_ratio']\n",
        "    sql_inc_efficiency = df_sql_inc['correlation'] / df_sql_inc['time_ratio']\n",
        "    \n",
        "    print(f\"   SQL Selection (Non-incremental) Average Efficiency: {sql_non_inc_efficiency.mean():.4f}\")\n",
        "    print(f\"   SQL Selection (Incremental) Average Efficiency: {sql_inc_efficiency.mean():.4f}\")\n",
        "    \n",
        "    print(\"\\n5. Strategy Recommendation:\")\n",
        "    if volume_efficiency.mean() > max(sql_non_inc_efficiency.mean(), sql_inc_efficiency.mean()):\n",
        "        print(\"   Recommended: Data Volume Strategy\")\n",
        "    elif sql_inc_efficiency.mean() > sql_non_inc_efficiency.mean():\n",
        "        print(\"   Recommended: SQL Selection (Incremental) Strategy\")\n",
        "    else:\n",
        "        print(\"   Recommended: SQL Selection (Non-incremental) Strategy\")\n",
        "    \n",
        "    print(\"\\nAnalysis Complete!\")\n",
        "\n",
        "print_analysis_summary(df_volume, df_sql_non_inc, df_sql_inc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_summary = {\n",
        "    'data_volume_analysis': df_volume.to_dict('records'),\n",
        "    'sql_non_incremental_analysis': df_sql_non_inc.to_dict('records'),\n",
        "    'sql_incremental_analysis': df_sql_inc.to_dict('records'),\n",
        "    'summary': {\n",
        "        'data_volume_efficiency': df_volume['correlation'].sum() / df_volume['time_ratio'].sum(),\n",
        "        'sql_non_inc_efficiency': df_sql_non_inc['correlation'].sum() / df_sql_non_inc['time_ratio'].sum(),\n",
        "        'sql_inc_efficiency': df_sql_inc['correlation'].sum() / df_sql_inc['time_ratio'].sum()\n",
        "    }\n",
        "}\n",
        "\n",
        "summary_file = \"fidelity_analysis_summary.json\"\n",
        "with open(summary_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Analysis results saved to: {summary_file}\")\n",
        "print(\"\\nAll analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "spark-test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
